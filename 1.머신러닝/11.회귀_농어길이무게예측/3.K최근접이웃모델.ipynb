{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minkowski'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Regression based on k-nearest neighbors.\n",
      "\n",
      "The target is predicted by local interpolation of the targets\n",
      "associated of the nearest neighbors in the training set.\n",
      "\n",
      "Read more in the :ref:`User Guide <regression>`.\n",
      "\n",
      ".. versionadded:: 0.9\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_neighbors : int, default=5\n",
      "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "\n",
      "weights : {'uniform', 'distance'}, callable or None, default='uniform'\n",
      "    Weight function used in prediction.  Possible values:\n",
      "\n",
      "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "      are weighted equally.\n",
      "    - 'distance' : weight points by the inverse of their distance.\n",
      "      in this case, closer neighbors of a query point will have a\n",
      "      greater influence than neighbors which are further away.\n",
      "    - [callable] : a user-defined function which accepts an\n",
      "      array of distances, and returns an array of the same shape\n",
      "      containing the weights.\n",
      "\n",
      "    Uniform weights are used by default.\n",
      "\n",
      "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "    Algorithm used to compute the nearest neighbors:\n",
      "\n",
      "    - 'ball_tree' will use :class:`BallTree`\n",
      "    - 'kd_tree' will use :class:`KDTree`\n",
      "    - 'brute' will use a brute-force search.\n",
      "    - 'auto' will attempt to decide the most appropriate algorithm\n",
      "      based on the values passed to :meth:`fit` method.\n",
      "\n",
      "    Note: fitting on sparse input will override the setting of\n",
      "    this parameter, using brute force.\n",
      "\n",
      "leaf_size : int, default=30\n",
      "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "    speed of the construction and query, as well as the memory\n",
      "    required to store the tree.  The optimal value depends on the\n",
      "    nature of the problem.\n",
      "\n",
      "p : int, default=2\n",
      "    Power parameter for the Minkowski metric. When p = 1, this is\n",
      "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "\n",
      "metric : str or callable, default='minkowski'\n",
      "    Metric to use for distance computation. Default is \"minkowski\", which\n",
      "    results in the standard Euclidean distance when p = 2. See the\n",
      "    documentation of `scipy.spatial.distance\n",
      "    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
      "    the metrics listed in\n",
      "    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
      "    values.\n",
      "\n",
      "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "    must be square during fit. X may be a :term:`sparse graph`, in which\n",
      "    case only \"nonzero\" elements may be considered neighbors.\n",
      "\n",
      "    If metric is a callable function, it takes two arrays representing 1D\n",
      "    vectors as inputs and must return one value indicating the distance\n",
      "    between those vectors. This works for Scipy's metrics, but is less\n",
      "    efficient than passing the metric name as a string.\n",
      "\n",
      "metric_params : dict, default=None\n",
      "    Additional keyword arguments for the metric function.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of parallel jobs to run for neighbors search.\n",
      "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "    for more details.\n",
      "    Doesn't affect :meth:`fit` method.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "effective_metric_ : str or callable\n",
      "    The distance metric to use. It will be same as the `metric` parameter\n",
      "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "    'minkowski' and `p` parameter set to 2.\n",
      "\n",
      "effective_metric_params_ : dict\n",
      "    Additional keyword arguments for the metric function. For most metrics\n",
      "    will be same with `metric_params` parameter, but may also contain the\n",
      "    `p` parameter value if the `effective_metric_` attribute is set to\n",
      "    'minkowski'.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_samples_fit_ : int\n",
      "    Number of samples in the fitted data.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "NearestNeighbors : Unsupervised learner for implementing neighbor searches.\n",
      "RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\n",
      "KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\n",
      "RadiusNeighborsClassifier : Classifier implementing\n",
      "    a vote among neighbors within a given radius.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "   neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      "   different labels, the results will depend on the ordering of the\n",
      "   training data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> X = [[0], [1], [2], [3]]\n",
      ">>> y = [0, 0, 1, 1]\n",
      ">>> from sklearn.neighbors import KNeighborsRegressor\n",
      ">>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      ">>> neigh.fit(X, y)\n",
      "KNeighborsRegressor(...)\n",
      ">>> print(neigh.predict([[1.5]]))\n",
      "[0.5]\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\python\\python310\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "KNeighborsRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def evaluate_reg_all(y_test, y_predict):\n",
    "    MSE = mean_squared_error(y_test,y_predict,squared=True)\n",
    "    RMSE = mean_squared_error(y_test,y_predict,squared=False)\n",
    "    MAE = mean_absolute_error(y_test,y_predict)\n",
    "    R2 = r2_score(y_test,y_predict)\n",
    "    \n",
    "    print(f'MSE: {MSE:.3f}, RMSE: {RMSE:.3F}, MAE: {MAE:.3F}, R^2: {R2:.3F}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9798660845532954\n",
      "0.9640250379099211\n",
      "MSE: 3541.047, RMSE: 59.507, MAE: 37.951, R^2: 0.964\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Data_study\\1.머신러닝\\11.회귀_농어길이무게예측\\3.K최근접이웃모델.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m x_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     [\u001b[39m50\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m y_predict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m label \u001b[39m=\u001b[39m labels[y_predict[\u001b[39m0\u001b[39;49m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m y_predict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(x_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Data_study/1.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/11.%ED%9A%8C%EA%B7%80_%EB%86%8D%EC%96%B4%EA%B8%B8%EC%9D%B4%EB%AC%B4%EA%B2%8C%EC%98%88%EC%B8%A1/3.K%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83%EB%AA%A8%EB%8D%B8.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m confidence \u001b[39m=\u001b[39m y_predict[\u001b[39m0\u001b[39m][y_predict[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margmax()]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 경고메세지 끄기\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "##########데이터 로드\n",
    "\n",
    "fish_df = pd.read_csv('./data/fish.csv')\n",
    "##########데이터 분석\n",
    "\n",
    "##########데이터 전처리\n",
    "# x_data = np.array([fish_df.drop(['무게'], axis=1)])\n",
    "x_data = fish_df.drop(['무게'], axis=1)\n",
    "y_data = fish_df['무게']\n",
    "# y_data = np.array(['무게'])\n",
    "# y_data = np.array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777)\n",
    "\n",
    "\n",
    "\n",
    "##########모델 생성\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3) #하이퍼파라미터 튜닝 k\n",
    "\n",
    "##########모델 학습\n",
    "\n",
    "result = model.fit(x_train, y_train)\n",
    "\n",
    "##########모델 검증\n",
    "\n",
    "print(model.score(x_train, y_train)) #0.9680392257494445\n",
    "print(model.score(x_test, y_test)) #0.9929281790592219\n",
    "\n",
    "y_predict = model.predict(x_test) # 테스트 데이터 예측\n",
    "evaluate_reg_all(y_test, y_predict) #테스트 데이터에 대한 평가\n",
    "\n",
    "##########모델 예측\n",
    "\n",
    "x_test = np.array([\n",
    "    [50]\n",
    "])\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "label = labels[y_predict[0]]\n",
    "y_predict = model.predict_proba(x_test)\n",
    "confidence = y_predict[0][y_predict[0].argmax()]\n",
    "\n",
    "print(label, confidence) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[참고]\n",
    "기계 학습에서 훈련 데이터보다 테스트 데이터에서 R-제곱(R2) 값이 더 높은 것은 드문 일입니다.\n",
    "이 비정상적인 시나리오에는 몇 가지 이유가 있을 수 있습니다.\n",
    "1. 데이터 유출\n",
    "2. 작은 테스트 세트\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
